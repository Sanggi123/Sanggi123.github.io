---
layout: single
title:  "[week11] NLP 기초 프로젝트2"
categories: Level2

---

# KLUE Wrap-up Report

# 프로젝트 개요

## A. 개요

문장의 단어에 대한 속성과 관계를 학습하여, 주어진 문장 속에서 단어 사이의 관계를 추론하는 모델을 학습 시킴. 이를 통해 자연어 문장에서 정보를 요약하고 중요 성분을 파악하기 위함.

## B. 개발환경

- **모델 학습 환경** : 1인당 V100 서버를 SSH로 연결해서 사용
- **협업 환경** : Github, Notion
- **소통 환경** : Zoom, Slack

---

# 프로젝트 팀 구성 및 역할

|           | 역할 및 시도 내용                                                                |
| --------- | ------------------------------------------------------------------------- |
| 김기호_T6013 | EDA, Data Preparation, Fine-tuning, Hyperparameter Tuning                 |
| 박상기_T6057 | EDA, Data Preparation                                                     |
| 방신근_T6068 | 개발환경구축, Paper research, Custom model 제작, Hyperparameter Tuning            |
| 심재혁_T6093 | Entity token adder제작, Entity type restriction, Binary classifier          |
| 황순열_T6193 | Model Selection, Ensemble, Hyperparameter Tuning, LLAMA-7b를 활용한 추론(폐기)    |
| 김건우_T6197 | TAPT, dynamic padding, input prompt, gradient accumulation, data cleaning |

---

# 프로젝트 수행 절차 및 방법

## 1. EDA

- **Data Understanding**
  
  1. **KLUE 논문 리뷰**: KLUE 논문을 읽어보며 데이터셋이 만들어진 배경, 데이터 수집 및 구축 방법, 평가 지표 등을 파악하였다.
  2. **data description 작성**: KLUE 논문의 읽고 이해한 내용을 바탕으로 data description을 작성하여 데이터셋에서 각 column과 label이 나타내는 의미를 정리하였다. 이러한 과정을 통해 데이터셋의 다양한 특성들을 알 수 있었고, 이는 향후 데이터 분석과 모델링 전략을 수립하는 데 도움이 되었다.

- **Data Analysis**
  
  1. **label 분포:** 30개의 label에 대해서 불균형한 분포를 보였다. 따라서, 이러한 불균형을 해소하기위해 다양한 기법들을 적용해 봐야겠다는 생각이 들었다.
     
     ![label.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\fac652581f90e49f87834ad4aabe7e9a1a4b2284.png)
2. **source 분포:** 학습 데이터에는 wikipedia에서 수집한 데이터가 많았고, 평가 데이터에는 대체로 비슷하나 wikitree에서 수집한 데이터가 조금 더 많았다. 
   
     ![Untitled.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\ab72ff4b2c52482c617859623338077aa22fc2b5.png)

3. **subject type 분포:** 학습 및 평가 데이터에서 subject entity type 분포를 확인한 결과 두 데이터셋이 유사한 분포를 보였다.
   
     ![Untitled 1.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\0ae0a39b40c9812c09f97b77c6b74fe55a6f065f.png)

4. **object type 분포:** 학습 및 평가 데이터셋에서 object entity type 분포를 확인한 결과, 두 데이터셋이 다른 분포를 보였다. 학습 데이터셋에는 PER과 ORG인 데이터가 많았지만, 평가 데이터셋에는 POH인 데이터가 많았다. 따라서, 데이터를 증강하거나 처리할 때 이러한 분포의 차이를 고려해 봐야겠다는 생각이 들었다.
   
   ![Untitled 2.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\19cedc5c0918c37d588513e56726bf1abe827a4f.png)

5. **entity pair(no_relation 제외) 분포:**  subject entity type의 고윳값은 ORG, PER, object entity type의 고윳값은 ORG, PER, POH, LOC, DAT, NOH로 총 12가지의 entity pair type이 존재했다. 하지만 no_relation일 때를 제외하고 label이 나타내는 의미를 고려했을 때, entity pair type이 PER-NOH인 경우에는 이를 만족하는 적절한 label이 존재하지 않았다. 하지만 label이 no_relation이 아닌 데이터에 한해 entity pair type 분포를 조회했을 때 PER-NOH 쌍의 데이터가 일부 존재했다. 따라서 일부 데이터에 한해서는 entity type 표기가 적절하지 않다고 판단하고 이를 데이터 전처리 과정에서 고려해 봐야겠다는 생각이 들었다.
   
   ![Untitled 3.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\90a73feff332b084ab462f08fea63c8d4fc785e8.png)

## 2. Data Preparation

- **Data** **Preprocessing**
  
  - **기존 데이터 column 수정**
    - 기존에는 **‘id’, ‘sentence’, ‘subject_entity’, ‘object_entity’, ‘label’, ‘source’**로 구성되어 있었으나, entity에 관한 정보를 조금 더 편리하게 모델에 입력하기위해 다음과 같이 데이터 column을 재구성했다.
    - **‘id’, ‘sentence’ , ‘label’, ‘source’, ‘subject_word’, ‘subject_start_idx’, ‘subject_end_idx’, ‘subject_type’, ‘object_word’, ‘object_start_idx’, ‘object_end_idx’, ‘object_type’, ‘entity_pair’**

- **Data Cleaning**
  
  - **중복 데이터 제거**
    
    - train data 중 ‘**sentence’, ‘subject_entity’, ‘object_entity’, ‘label’**이 모두 같은 데이터가 84개 존재해서 제거했다. **32470개 → 32428개로 수정**
    - 위 데이터들을 제거하고 나서 train data 중 ‘**sentence’, ‘subject_entity’, ‘object_entity’**이 모두 같은 데이터가 10개 존재했고, 이들은 **label**만 달랐기에 둘 중 옳다고 생각되는 데이터들을 남기고 나머지는 제거했다. **32428개 → 32425개로 수정**
  
  - entity type 수정
    
    ```
    예시)
    1. per:schools_attended
    subject_type: {'PER': 82}
    object_type: {'ORG': 80, 'LOC': 2}
    2. org:top_members/employees
    subject_type: {'ORG': 4283, 'PER': 1}
    object_type: {'PER': 4196, 'POH': 52, 'ORG': 22, 'LOC': 13, 'NOH': 1}
    ```
    
    - 학습 데이터에서 label마다 entity pair 갯수를 출력해봤을 때, 위의 예시처럼 타입 표기가 적절치 않은 데이터들이 존재했다. 실제로 이러한 데이터들을 확인했을 때 대부분의 데이터가 잘못 표기된 데이터였다. 따라서 예시에서 1번의 per:schools_attended label처럼 majority type이 유일한 경우 entity type을 majority type으로 수정했고, 2번의 org:top_members/employees처럼 majority type이 유일하지 않은 경우 EDA 과정에서 평가 데이터에 POH type이 많았던 것을 고려해 entity type을 POH로 수정했다.

- **Data Augmentation**
  
  - **Back Translation**
    
    - text = "1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8번으로 배정되었다.”
    - backtranslation = '그가 1967년 요미우리 자이언츠에 1위를 차지하면서 합류했다. 프로야구 드래프트 1개, 그의 번호가 1위에 배정됐다. 8번.’
    - 다음과 같이 문장의 의미가 너무 달라져서 사용하지 않았다.
  
  - **Pororo 라이브러리 활용**
    
    - Fill in the blanck(fib) 사용
      
      ![Untitled 4.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\d764ad10a734599512b8b86972741fbc581363e4.png)
    
    - 다음과 같이 빈칸에 적절한 단어를 채우지만 적절한 빈칸의 위치를 찾기 어렵고 의미가 달라지는 경우가 발생하여 사용하지 않았다.

- **문장 내 단어 Masking**
  
  - Bert 모델이 MLM(Masked Language Model)을 활용한 모델이다. 따라서 문장 내 임의의 단어를 Masking을 하고 학습을 진행하면 좀 더 문장을 이해 할 것이라고 가정
  - micoro f1 score : 70.8098 → 74.5562
  - f1 score에서 많은 향상을 얻을 수 있었다. 따라서 최종 모델에 활용하였다.

- **Entity Swap**
  
  - label이 서로 상반관계가 존재(members ↔ member of 등) 한다.
  - 그 label의 entity를 swap 하여 label을 변경해주고 데이터 증강에 활용
  - 시간이 부족하여 테스트 데이터의 성능확인은 하지 못했으나, 검증 데이터에 대한 성능 향상이 약 1% 있었다. 따라서 최종 모델에 활용하였다.

## 3. Model Selection

- **Model Selection**
  
  - `**klue/roberta-large**`
    
      KLUE 데이터 셋을 사전 학습한 RoBERTa-Large 모델 
  
  - `**xlm-roberta-large**`
    
      다국어를 사전 학습 시킨 교차 언어 모델

- **Focal Loss**
  
  - 데이터 셋의 불균형 때문에 도입한 손실 함수.
  
  - 대부분의 label은 “no_relation”으로 학습에 기여하지 않는 easy negative이므로 학습에 비효율적이다. easy negative는 높은 확률로 객체가 아님을 잘 구분할 수 있으며, 이는 각각의 loss 값이 작음을 의미한다. 하지만 비율은 굉장히 높기에 전체 loss 및 gradient를 계산할 때, 주는 영향이 매우 커지는 문제가 발생한다.
  
  - 따라서 확률이 높은 클래스에 대해서는 확률이 낮은 클래스보다 loss를 더 낮춰주는 방식으로 보정해준다.
    
    $$
    F_{Loss} = \alpha (1-p)^\gamma \times CE_{Loss}
    $$

## 4. Model Fine-tuning

| Model Fine-tuning                                                                                                         | Description                                                                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| #1. xlm-roberta-large                                                                                                     | 100개 언어에 대해 사전학습된 모델을 그대로 사용                                                                                                                                                                                                                         |
| #2. klue/roberta-large + input prompt + TAPT                                                                              | TAPT를 수행한 모델을 불러와 input의 형태를 “[CLS] subject와 object의 관계 [SEP] sentence [SEP]”로 입력한 모델                                                                                                                                                                |
| #3. klue/roberta-large + custom layer + binary + general                                                                  | 모든 데이터에 대하여 binary분류기를 이용하여 양성/음성 레이블을 먼저 분류하고, 양성 레이블로 분류된 데이터에 한해 general 분류기를 이용하여 세부 레이블로 분류하는 모델                                                                                                                                                |
| #4. klue/roberta-large + custom model + semantic typing                                                                   | CLS 토큰, subject entity 토큰과 object entity 토큰의 hidden state를 활용하였고, 원문장 앞에 entity의 정보를 자연어 형태로 넣어준 모델                                                                                                                                                  |
| #5. klue/roberta-large + custom model(typed entity marker(punct), entity hidden states)                                   | subject/object entity의 첫번째 entity marker의 hidden state를 classification에 활용했고, entity type을 한국어로 입력해서 sentence에 추가한 모델                                                                                                                                |
| #6.  klue/roberta-large + custom model(typed entity marker, CLS & entity hidden states, additional fully connected layer) | sentence를 An Improved Baseline for Sentence-level Relation Extraction 논문의 Typed entity marker 방법에 따라 모델의 입력에 넣어주었고, 앞 부분에 [SEP] 토큰을 추가하여 entity pair의 word를 추가한 모델. 추가로 CLS와 entity hidden states를 활용하기 위해 두개의 fully connected layer를 추가하여 분류기에 넣었다. |

- **Custom Model Development**
  
  - **Hidden State 활용**
    
      baseline의 model architecture를 개선할 수 있는 방법을 리서치하던 중, *Matching the Blanks: Distributional Similarity for Relation Learning (2019)*을 발견하게 되었고, 기존의 [CLS] 토큰만을 활용하여 classification을 하는 것보다, subj entity와 obj entity의 hidden state를 활용하여 classification을 하는 것이 더 성능이 좋다는 것을 확인할 수 있었다. 아래의 이미지는 위 논문에서 시도한 모델 아키텍쳐 목록. baseline의 아키텍쳐는 (a)에 해당하고, 논문의 실험에서 성능이 가장 높았던 아키텍쳐는 (f)에 해당한다.
    
    ![Untitled 5.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\be58a519dd9d5eff4f75ea9df0053f410bedee53.png)
    
    ![Untitled 6.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\ca3c45b149e90af2bf0d232b11745f872bda0fa8.png)
    
      klue 데이터 셋에 해당 내용(f)를 적용한 결과 다음과 같이 전반적인 성능 향상을 확인할 수 있었다. 1.2.9가 hidden state 활용한 결과고, 두 실험 모두 Klue/bert-base를 모델로 사용했다.
    
    ![Untitled 7.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\15c53e04db7b9a268ae98cb36667d4d1993d78aa.png)
    
    | Model           | micro f1 score |
    | --------------- | -------------- |
    | Baseline        | 83.749         |
    | hidden state 활용 | 84.778         |

- **Entity type 추가**
  
    Entity type이 label 추론에 큰 영향을 미친다고 판단해서, entity type 정보를 학습 과정에 도입하고자 했다. An Improved Baseline for Sentence-level Relation Extraction(2022) 논문을 참고해서 아래와 같이 다양한 방법으로 entity type을 추가해보았다.
  
  ![Untitled 8.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\84433513792fd6ac792c563a57a1885907e78eea.png)
  
      위 모델에서 #4번 모델은 Typed entity marker (punct) 형식으로 type을 sentence에 입력한 것이고, #5번 모델은 Entity marker과 한글로 번역한 type입력한 것이다. 그리고, #6번 모델은 Typed entity marker 형식으로 type을 sentence에 입력했다.

- I**nput prompt**
  
    baseline에서는 **“[CLS] subject_word [SEP] object_word [SEP] sentence [SEP]”**로 모델의 입력으로 주었지만, 이는 [SEP] 토큰 수가 많이 들어가 모델에게 혼동을 줄 수 있다고 생각했다. 그래서 아래와 같은 여러 형태를 입력으로 줘서 성능을 비교했다.
  
  1. **[CLS] subject_word와 object_word의 관계 [SEP] sentence [SEP]**
  2. **[CLS] 다음 문장에서 subject_word와 object_word의 관계 [SEP] sentence [SEP]**
  3. **[CLS] sentence [SEP] subject_word와 object_word의 관계 [SEP]**
  4. **[CLS] 다음 문장에서 subject_word와 object_word의 관계는 무엇인가? [SEP] sentence [SEP]**
  5. **[CLS] subject_word와 object_word의 관계는 무엇인가? [SEP] sentence [SEP]**
  - **실험 결과**
    
        | Model | eval micro f1 score | eval auprc | test micro f1 score | test auprc |
        | --- | --- | --- | --- | --- |
        | Baseline | 84.7 | 78.448 | 71.4770 | 70.5767 |
        | prompt1 | 84.841 | 78.625 | 71.6792 | 69.2448 |
        | prompt2 | 84.794 | 79.272 | 낮아서 제출x | 낮아서 제출x |
        | prompt3 | 84.855 | 78.931 | 70.2239 | 71.6504 |
        | prompt4 | 84.894 | 80.141 | 70.2239 | 71.6504 |
        | prompt5 | 84.792 | 79.753 | 낮아서 제출x | 낮아서 제출x |
    
    - **결과 해석**
      - baseline과 비교했을 때, 전체적으로 성능 향상이 있었으며 그 중에 가장 좋은 성능을 보인 prompt1을 사용했다.

- **TAPT**
  
    baseline에서 사용된 klue/bert 모델은 대량의 말뭉치를 학습했기에 일반화 성능을 확보했지만, 특정 도메인에서 사용하기엔 제대로 성능을 발휘하지 못하고, fine-tuning만으로는 한계가 존재했다. 각 task를 수행하고자 할 때, 모델을 해당 도메인 or task의 데이터로 다시 학습 시키는 TAPT를 사용해 parameter를 target task에 더 집중되도록 학습 시킬 필요가 있다.
  
    TAPT를 진행하기 위해 train 데이터와 test 데이터를 합쳐서 사용했으며, pretrain이기에 label를 사용하지 않고 evaluation을 수행하지 않았다. 
  
  - **실험 결과**
    
        | Model | eval micro f1 score | eval auprc |
        | --- | --- | --- |
        | not TAPT | 84.946 | 79.575 |
        | TAPT | 84.761 | 80.302 |
    
    - **결과 해석**
      - TAPT를 적용했지만, 유의미한 변화를 주지 못했다. 이는 TAPT를 진행할 때, 입력으로 정답 데이터를 추가해주지 못해서 변화를 주지 못한 것으로 추측된다.

- **Entity Type Restriction**
  
    baseline의 dataset에서 Entity type pair(subject entity type - object entity type)별로 나올 수 있는 relation이 한정되어 있어 *‘Relation Classification with Entity type Restriction (2021)’* 논문의 개념을 참고하여 성능 향상을 꾀했다. Entity type pair별로 별도의 classifier(논문에서는 specific classifier)를 제작하여 pair별로 다른 분류기를 이용하게 하였다.
  
  <img src="file:///D:/github-blog/Ahzic.github.io/Sanggi123.github.io/assets/images/1b7032b6d49c72074cebad91d0049bbc7cc7f320.png" title="" alt="Untitled 9.png" width="600">
  
    따라서 위의 개념을 바탕으로 기존 왼쪽 그림(baseline)과 같은 구조를 오른쪽 그림과 같은 구조로 변환하여 fine-tuning을 수행하였다.
  
  ![Untitled 10.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\49a9a04f9bd5ce8a9486db0ad8a358f16021f353.png)

## 4. Resource Optimization

- **Mixed Precision**
  
    forward와 backward 과정에서는 FP16으로 계산하고, 가중치를 업데이트하는 과정에서 다시 FP32로 변환하는 과정을 거쳐서 계산량과 메모리 사용량을 줄이면서 동시에 FP16으로 연산하는 과정에서 발생하는 정보의 손실을 최소화할 수 있다.
  
    이번 프로젝트에서는 [huggingface 공식문서](https://huggingface.co/docs/transformers/main_classes/trainer)를 참고해 TrainingArguments에 있는 `fp16 = True` 로 설정해서 Mixed Precision을 사용했다.

- **Dynamic Padding**
  
    기본적으로 모든 데이터에 대한 sequence 길이를 동일하게 설정하지만, Dynamic Padding은 batch 단위로 한 batch 내의 sequence 최대 길이를 기준으로 padding 토큰을 넣는 기법이다. 
  
    이를 통해 max_length만큼 일괄적으로 길이를 정할 때보다 적은 padding 토큰을 추가하게 되어 GPU에서의 연산량을 감소시키는 효과가 있었다. 
  
    이번 프로젝트에서는 [huggingface 공식문서](https://huggingface.co/docs/transformers/main_classes/data_collator)를 참고해 `DataCollatorWithPadding`을 사용했다.

- **Gradient Accumulation**
  
    기존의 학습은 하나의 mini-batch를 거친 후 optimizer를 통해 학습을 진행한다. 하지만 gradient accumulation은 여러 개의 mini-batch를 거친 후에 누적된 값을 한 번에 학습한다. 이를 통해 더 큰 batch size를 사용하는 것과 같은 효과를 낼 수 있다. 
  
  ![11.png](D:\github-blog\Ahzic.github.io\Sanggi123.github.io\assets\images\2fe7bf8816dd99b0b67f1f55e0ef4c27257c7c2f.png)
  
    TrainingArguments에 있는 `gradient_accmulation_steps`에 값을 지정해서 사용했다.

## 5. Hyperparameter Tuning

- Optuna를 활용해 찾은 최적의 하이퍼 파라미터는 아래 표와 같다.
- 모델 넘버는 프로젝트 수행 절차 및 방법의 모델 순서와 동일하다.

|                   | Model #1               | Model #2               | Model #3 | Model #4 | Model #5 | Model #6 |
| ----------------- | ---------------------- | ---------------------- | -------- | -------- | -------- | -------- |
| learning_rate     | 0.00000992925489849659 | 1.0343185256317574e-05 | 1e-5     | 1e-5     | 5e-5     | 1e-5     |
| adam_epsilon      | 0.00000022212066891763 | 1e-8                   | 1e-8     | 1e-8     | 1e-8     | 1e-8     |
| adam_beta1        | 0.8303720723968628     | 0.9                    | 0.9      | 0.9      | 0.9      | 0.9      |
| adam_beta2        | 0.9831950743948984     | 0.999                  | 0.999    | 0.999    | 0.999    | 0.999    |
| warmup_steps      | 115                    | 384                    | 1000     | 350      | 500      | 1000     |
| weight_decay      | 0.12673903014035376    | 0.14115743665194863    | 0.01     | 0.1      | 0.01     | 1000     |
| lr_scheduler_type | linear                 | cosine                 | warmup   | warmup   | warmup   | linear   |
| seed              | 42                     | 42                     | 42       | 42       | 42       | 42       |

## 6. Single Model Evaluation

- 각 모델을 loss가 수렴할 때 까지 학습하고 성능을 비교하였다.

| Model | Micro f1 Score | AUPRC   |
| ----- | -------------- | ------- |
| #1    | 71.054         | 67.117  |
| #2    | 71.5207        | 70.5604 |
| #3    | *N/A           | *N/A    |
| #4    | 74.5562        | 78.4869 |
| #5    | 72.171         | 73.147  |
| #6    | 73.2288        | 78.3376 |

- 모든 모델들의 성능이 Micro F1 score 기준 70~75에 분포되었다.
- 단일 모델 기준 가장 성능이 좋았던 모델은 #4이다.

**여러 모델을 조합하여 사용. 위 표의 수치는 조합 된 **단일 모델 별 성능**이므로, 단일 모델 만을 사용하여 성능을 낸 다른 모델들과 직접적인 성능 비교가 어려움.*

## 7. Model Ensemble

- **Soft Voting**: 각 모델이 낸 prop 값을 평균하여 가장 높은 prop값의 라벨로 선정
- **Hard Voting**: 각 모델이 뽑은 라벨중 다수인 라벨을 선정

| 조합                     | 결과(micro f1) | 결과(auprc) |
| ---------------------- | ------------ | --------- |
| #1, #2, #3, #4, #5, #6 | 75.3534      | 81.3325   |
| #1, #2, #3, #4, #6     | 75.3263      | 80.7520   |
| #1, #2, #4, #5, #6     | 75.7323      | 81.0962   |
| #2, #4, #5, #6         | 75.3523      | 81.4638   |
| #1, #2, #4             | 75.8557      | 80.4      |
| #77                    | 75.3534      | 81.3325   |
| #78                    | 73.3617      | 76.8276   |
| #79                    | 75.4902      | 80.3618   |
| #80                    | 73.9471      | 79.3778   |
| #81                    | 75.5999      | 79.9988   |

- 앙상블 조합에 따라 미세한 성능 차이를 보였으며, 가장 좋은 성능을 보인 조합은 모델 **#1, #2, #4** 조합이었다.
  - 이는 앙상블에 사용된 모델이 다양할 수록 성능 향상에 도움이 된다는 가설에 부합하는 경우다. **#1**은 사전학습 데이터셋이 유일하게 다른 모델이었고, 나머지는 KLUE 데이터셋에 학습된 모델인데, **#2와 #4**는 그 중 가장 높은 성능을 보이면서 다른 파인튜닝 기법이 사용된 모델이었다. 즉, 성능이 무조건적으로 좋은 순으로 앙상블하는 것 보다 다양한 모델을 조합으로 사용하는 것이 더 높은 성능을 보여주었다.

---

# 프로젝트 수행 결과

- 5등🥉

|               | micro f1 | auprc   |
| ------------- | -------- | ------- |
| public score  | 75.8557  | 80.4636 |
| private score | 75.0315  | 82.0158 |

---