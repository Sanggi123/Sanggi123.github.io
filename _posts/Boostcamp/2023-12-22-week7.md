---
layout: single
title:  "[week7] NLP 기초 프로젝트"
categories: Level1

---

<br/>

# 기초 프로젝트 수행 리포트

## 프로젝트 개요

- 문장간 유사도 측정
  
  복수의 문장에 대한 유사도를 측정하는 Task 이다. 우리는 주어진 Semantic Text Similarity (STS) 데이터셋을활용하여 두 문장의 유사도르 측정하는 AI 모델을 수정하고, 구축해야한다. 최종 목표는 유사도를 0~5사이의 실수의 점수를 예측하는것이다.

- 평가 방법
  
  이 프로젝트의 평가 매트릭은 피어슨 상관 계수를 사용한다. 실수를 예측해야 하는 Regression 문제이기도 하지만, 정답과 예측의 선형 관계 즉, 예측값이 정답에 일치하지 않더라도증감율이 같으면 잘 예측했다고 보는 것이다. 이는 전체적인 경향을 잘 예측하는 것이 중요하다.

## EDA

- Train 데이터를 시각화하여 **source의 분포, source별 label 분포, source별 binary label 분포, label의 분포, binary label의 분포를 확인하였다.**
  - **source의 분포**
    - sampled 데이터의 수가 rtt 데이터의 수보다 약 2배 많았다.
  - **source별 binary label 분포**
    - sampled 데이터에는 0, rtt 데이터에는 1의 값이 많았다. 검증 데이터에도 같은 경향이 보였다.
  - **label의 분포**
    - 0.0 ~ 0.5의 값이 압도적으로 많았고, 4.5 ~ 5.0의 데이터 값이 많이 부족했다.
  - **binary label의 분포**
    - 0값이 1값 보다 많았다.
  - **source별 label 분포**
    - sampled 데이터에서 0.0 ~ 0.5의 값이 많이 분포한 것을 확인했다.
- 결론
  - Train 데이터 셋에서 데이터의 불균형을 확인 할 수 있었고, 그 중 source의 sampled 데이터에서 0.0 ~ 0.5의 데이터가 너무 많아 다운 샘플링을 통해 균형을 어느 정도 맞추기로 했다.
  - 다운 샘플링을 어느 정도 하더라도 불균형이 존재하기 때문에 데이터 증강을 활용해야 한다고 생각 했다.

## Data Preprocessing

- **수행 방법**
  
  1. **soynlp**를 사용하여 ‘ㅋㅋㅋ’, ‘ㅎㅎㅎ’와 같은 반복 문자열 축약
  2. **정규 표현식**을 사용하여 ‘!!!’, ‘…..’와 같은 반복되는 특수문자 축약
  3. **pykospacing**을 사용하여 띄어쓰기 수정
  4. **hanspell**을 사용하여 맞춤법 수정

- **수행 결과**
  
  반복 문자열을 축약하는 방법과 띄어쓰기를 수정하는 방법은 성능 향상이 미미했다. hanspell을 통한 맞춤법 수정은 정확하게 수정되지 않고, 다른 의미로 바뀌게 수정되는 등 데이터 의미가 변하여 오히려 성능이 떨어졌다.

## Data Augmentation

- **수행 방법**
  
  1. sentence swap을 통해 데이터 증대
  2. pororo 기반 문장 패러프레이징을 사용하여 유사한 문장을 생성
  3. 다운 샘플링 할 때, 버리는 데이터를 활용하여 같은 문장을 복제한 뒤, 5.0 label의 데이터로 증대

- **수행 결과**
  
  sentence swap을 통해 0.01% ~ 0.02% 정도의 성능 향상을 얻을 수 있었다. pororo를 사용시 성능 향상이 약간 있었으나, 떨어지는 경우도 있었다. pororo 사용시 원래 문장의 의미가 변했을 가능성을 생각했다.

## 모델 수정

- **수행 방법**
  
  각 데이터의 source 값을 토큰화 하여 모델의 스페셜 토큰을 추가하였다.

- **수행 결과**
  
  성능이 약간 상승한 것을 확인할 수 있었다. 각 데이터의 source 정보를 통해 source의 해당하는 문장 의미를 어느정도 예측할 수 있어 성능이 향상한 것 같다.

## 고찰

여러가지의 실험을 통해 좋은 성능은 얻지 못했다. 그렇다고 한 것들이 무의미하다고 생각하지 않는다. 데이터의 전처리를 통해서 loss값의 안정화, 에폭 수에 따른 매트릭 성능의 향상 등 여러 관점에서 의미가 있는 결과를 얻었기 때문에 나름 의미가 있다고 생각한다. 성능이 좋지 않다고 포기하지 않는 것이 아니라 다른 관점에서 살펴볼 필요가 있다고 깨달았다.

## 한계점 & 아쉬운점

한글인 데이터를 전처리하는 것이 너무 까다로웠다. 증강을 위해 조금만 수정해도 그 데이터의 의미가 완전히 바뀌어 버리기 때문이었다. 새삼 세종대왕의 위대함을 깨닫는 계기가 되었다.

binary-label을 활용해서 모델을 수정 할려고 했지만, 너무 어려웠다. 하나를 수정하면 두 개의 오류가 나오고.. 오류가 점점 증식했다. 그래도 오류를 수정하고 모델을 돌리면 결과가 처참했다. 어딘가에서 잘못되었단 것이다. 이처럼 모델을 알맞은 Task에 맞게 수정을 하지 못한 것이 너무 아쉬웠고, 다음 프로젝트에서 한번 시도해볼 생각이다.
