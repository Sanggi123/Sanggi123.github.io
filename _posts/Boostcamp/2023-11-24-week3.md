---
layout: single
title:  "[week3] Deep Learning Basics 2"
categories: Level1

---

# 어제 공부한 내용📝

(어제 작성해야 했지만, 깜빡했다.. )



- # RNN

- # Transformer

- # Generative Model

- # Data Visualiztion





# 오늘 공부한 내용📝

일단 강의 진도가 다 나가서 오늘은 개인 공부를 해야한다. 그렇지만 블로그에 뭐라도 쓰고 싶어 Further Questions에 대한 생각을 정리했다.



- Regression Task와 Classification Task의 loss function이 다른 이유는?
  
  회귀문제의 목적은 연속적인 값을 예측하는 것이다. 따라서 실제 값과 예측 값 사이의 차이를 구해야 하기 때문에 **MSE**를 사용해서 오차를 반연하는 것이 일반적이라고 생각한다. 
  
  분류문제의 목적은 주어진 입력이 어떤 범주에 속하는지 예측하는 것이다. 따라서 예측한 확률 분포에서 실제 레이블 분포 사이의 차이를 구해야하기 때문에 **Cross-Entropy Loss** 를 일반적으로 사용하는 것 같다.
  
  **Cross-Entropy Loss**이 확률 분포와 관련이 있는 것은 알지만 자세히 몰라 공부를 좀 해야겠다.
  
    

- CNN 모델이 Sequential Data를 처리하는 데에는 어떤 한계점이 있는가
  
  CNN은 고정된 크기의 커널을 가중치로 사용하기 때문에 로컬 패턴을 학습하는데 좋다. 순차 데이터는 시간에 따라 패턴이 변화하기 때문에 CNN에서 순차 데이터를 처리하기에 어려울 수 있다.
  
    

- RNN, LSTM, GRU의 구조적 차이점이 무엇인지 각각 장단점이 무엇인가
  
  RNN - 가장 기본적인 순환 신경망이다. 시퀸스 길이에 상관없이 입력을 처리할 수 있지만, 장기 의존성 문제를 학습하기 어렵다.
  
  LSTM - 3개의 게이트를 활용하여 장기 의존성 문제를 해결하기 위해 설계되었다. 따라서 장기 의존성 문제를 어느정도 해결했지만, 계산 비용이 높고, 파라미터가 많아 학습이 느리다.
  
  GRU - LSTM의 간소화 버전이라고 볼 수 있다. 구조가 간단하여 계산 효율성이 높다. 하지만, 정보를 유지해야하는 기간이 늘어난다면 LSTM 보다 성능이 안 좋을 수 있다.
  
    

- Transformer의 구조에서 Query, Key, Value가 각각 어떤 역할을 하는가
  
  Query - 분석하고자 하는 단어를 말하고, 그 단어에 대한 Query 벡터가 생성된다.
  
  Key - 비교 대상이 되는 단어를 말하고, 입력된 시퀸스의 모든 단어를 말한다. 즉, 모든 단어에 대한 가중치 벡터이다. 각 단어에 대한 Key 벡터가 생성되고, Q와 K 사이의 유사도를 계산을 하는데 사용된다.
  
  Value - Key의 의미를 나타내는 가중치 벡터이고, 유사도를 계산한 Attention Score가 Value에 가중치를 부여하는 역할을 한다.
  
    

<br/>

# 참고 사이트

1. [RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)

2. [The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/)

3. [Auto Regressive Models](https://ratsgo.github.io/generative%20model/2018/01/31/AR/)

4. [[1906.02691] An Introduction to Variational Autoencoders](https://arxiv.org/abs/1906.02691)



<br/>

# 회고🤔

Generative Model에 관심이 조금 있는데.. 너무 어렵다. 일단 이해하기도 약간 어려운데 수식까지 막 나오니깐 도통 모르겠다. 

일단, transformer를 공부한 후에 다시 공부해봐야 겠다. 

week3는 배우는 맛이 있는 한 주였다. 

멘토링 시간에 gpt에 대한 얘기를 나눴는데, 흥미로운 얘기였다. gpt가 나오고 나서 시대가 많이 변했다는 것을 많이 느꼈고, 노코드의 시대가 점점 다가오는 것 같고, AI는 그저 강력한 툴이 될 것이고.. 앞으로 AI에 엄청 막 매달리지 말아야겠다는 생각이 들었다. 좀 더 다양하고 다른 시각에서 바라봐야겠다.
